{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"glove.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5GjKXet5/GHNlWvM0tBBb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qchf0QVBOAG","executionInfo":{"status":"ok","timestamp":1607274705146,"user_tz":300,"elapsed":253,"user":{"displayName":"Hatsune Morioka","photoUrl":"","userId":"00224534620695216442"}},"outputId":"0925c10b-39e8-41d7-93ee-d3881a218938"},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","#from glove import Corpus, Glove\n","\n","vectors_dict = {}\n","with open(\"glove.6B.50d.txt\") as book:\n","    for line in book:\n","        key = line.split(' ')[0]\n","        val = line.split(' ')[1:]\n","        for v in range(50):\n","            val[v] = float(val[v])\n","        vectors_dict[key] = val\n","book.close()\n","\n","joke_file = pd.read_csv('comedy.csv', sep=',')\n","jokes = joke_file.content\n","\n","count_vect = CountVectorizer()\n","jokes = count_vect.fit_transform(jokes)\n","jokes = jokes.toarray()\n","\n","features = count_vect.get_feature_names()\n","vectors_array = np.array([])\n","for i in range(len(features)):\n","    f = features[i]\n","    if f in vectors_dict.keys():\n","        vectors_array = np.append(vectors_array, vectors_dict[f])\n","    else:\n","        jokes = np.delete(jokes, i, 1)\n","\n","print(np.shape(jokes))\n","print(np.shape(vectors_array))\n","glove_X = np.multiply(jokes, vectors_array)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.arrays.numpy_.PandasArray'>\n"],"name":"stdout"}]}]}